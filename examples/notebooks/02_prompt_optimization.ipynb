{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Prompt Optimization\n",
    "\n",
    "This notebook explores advanced features of promptomatix including:\n",
    "- Different optimization backends\n",
    "- Various task types\n",
    "- Custom synthetic data generation\n",
    "- Advanced configurations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed the basic usage notebook first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import promptomatix functions\n",
    "from promptomatix.main import process_input\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Different Optimization Backends\n",
    "\n",
    "promptomatix supports multiple optimization backends. Let's compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different backends with the same input\n",
    "test_input = \"Summarize a long text in 2-3 sentences\"\n",
    "\n",
    "backends = [\"simple_meta_prompt\", \"dspy\"]\n",
    "results = {}\n",
    "\n",
    "for backend in backends:\n",
    "    print(f\"\\nüîÑ Testing {backend} backend...\")\n",
    "    \n",
    "    config = {\n",
    "        \"raw_input\": test_input,\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model_api_key\": api_key,\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"backend\": backend,\n",
    "        \"synthetic_data_size\": 3,\n",
    "        \"task_type\": \"summarization\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = process_input(**config)\n",
    "        results[backend] = result\n",
    "        \n",
    "        print(f\"‚úÖ {backend} completed\")\n",
    "        print(f\"üìù Optimized prompt:\")\n",
    "        print(result['result'])\n",
    "        print(f\"üí∞ Cost: ${result['metrics']['cost']:.4f}\")\n",
    "        print(f\"‚è±Ô∏è  Time: {result['metrics']['time_taken']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {backend} failed: {str(e)}\")\n",
    "        results[backend] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Task Types\n",
    "\n",
    "Let's explore various task types that promptomatix can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different task types and their inputs\n",
    "task_examples = {\n",
    "    \"classification\": {\n",
    "        \"input\": \"Classify emails as spam or not spam\",\n",
    "        \"description\": \"Binary or multi-class classification tasks\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"input\": \"Summarize a research paper in 200 words\",\n",
    "        \"description\": \"Text summarization tasks\"\n",
    "    },\n",
    "    \"translation\": {\n",
    "        \"input\": \"Translate English text to Spanish\",\n",
    "        \"description\": \"Language translation tasks\"\n",
    "    },\n",
    "    \"qa\": {\n",
    "        \"input\": \"Answer questions about a given text\",\n",
    "        \"description\": \"Question-answering tasks\"\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"input\": \"Generate creative writing based on a prompt\",\n",
    "        \"description\": \"Text generation tasks\"\n",
    "    }\n",
    "}\n",
    "\n",
    "task_results = {}\n",
    "\n",
    "for task_type, task_info in task_examples.items():\n",
    "    print(f\"\\nüîÑ Testing {task_type} task...\")\n",
    "    print(f\"Description: {task_info['description']}\")\n",
    "    \n",
    "    config = {\n",
    "        \"raw_input\": task_info['input'],\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model_api_key\": api_key,\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"backend\": \"simple_meta_prompt\",\n",
    "        \"synthetic_data_size\": 2,\n",
    "        \"task_type\": task_type\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = process_input(**config)\n",
    "        task_results[task_type] = result\n",
    "        \n",
    "        print(f\"‚úÖ {task_type} completed\")\n",
    "        print(f\"üìù Optimized prompt:\")\n",
    "        print(result['result'])\n",
    "        print(f\"üìä Synthetic data count: {len(result['synthetic_data'])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {task_type} failed: {str(e)}\")\n",
    "        task_results[task_type] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Synthetic Data Generation\n",
    "\n",
    "Let's explore how synthetic data is generated and how we can control it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different synthetic data sizes\n",
    "sizes = [1, 5, 10]\n",
    "size_results = {}\n",
    "\n",
    "for size in sizes:\n",
    "    print(f\"\\nüîÑ Testing with {size} synthetic examples...\")\n",
    "    \n",
    "    config = {\n",
    "        \"raw_input\": \"Extract key information from customer reviews\",\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model_api_key\": api_key,\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"backend\": \"simple_meta_prompt\",\n",
    "        \"synthetic_data_size\": size,\n",
    "        \"task_type\": \"extraction\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = process_input(**config)\n",
    "        size_results[size] = result\n",
    "        \n",
    "        print(f\"‚úÖ Generated {len(result['synthetic_data'])} examples\")\n",
    "        print(f\"üí∞ Cost: ${result['metrics']['cost']:.4f}\")\n",
    "        print(f\"‚è±Ô∏è  Time: {result['metrics']['time_taken']:.2f}s\")\n",
    "        \n",
    "        # Show a sample of synthetic data\n",
    "        if result['synthetic_data']:\n",
    "            print(\"üìä Sample synthetic data:\")\n",
    "            for i, example in enumerate(result['synthetic_data'][:2], 1):\n",
    "                print(f\"  {i}. {example}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        size_results[size] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Configurations\n",
    "\n",
    "Let's explore advanced configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom model parameters\n",
    "print(\"üîÑ Testing with custom model parameters...\")\n",
    "\n",
    "advanced_config = {\n",
    "    \"raw_input\": \"Generate a professional email response\",\n",
    "    \"model_name\": \"gpt-3.5-turbo\",\n",
    "    \"model_api_key\": api_key,\n",
    "    \"model_provider\": \"openai\",\n",
    "    \"backend\": \"simple_meta_prompt\",\n",
    "    \"synthetic_data_size\": 3,\n",
    "    \"task_type\": \"generation\",\n",
    "    \"temperature\": 0.3,  # Lower temperature for more focused output\n",
    "    \"max_tokens\": 2000,  # Custom max tokens\n",
    "    \"train_ratio\": 0.8   # 80% for training, 20% for validation\n",
    "}\n",
    "\n",
    "try:\n",
    "    advanced_result = process_input(**advanced_config)\n",
    "    \n",
    "    print(\"‚úÖ Advanced configuration completed\")\n",
    "    print(f\"üìù Optimized prompt:\")\n",
    "    print(advanced_result['result'])\n",
    "    print(f\"üí∞ Cost: ${advanced_result['metrics']['cost']:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Time: {advanced_result['metrics']['time_taken']:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Advanced configuration failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison and Analysis\n",
    "\n",
    "Let's compare the results from different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare backend performance\n",
    "print(\"üìä Backend Performance Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Backend':<20} {'Cost':<10} {'Time':<10} {'Status':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for backend, result in results.items():\n",
    "    if result:\n",
    "        cost = result['metrics']['cost']\n",
    "        time = result['metrics']['time_taken']\n",
    "        status = \"‚úÖ Success\"\n",
    "    else:\n",
    "        cost = time = 0\n",
    "        status = \"‚ùå Failed\"\n",
    "    \n",
    "    print(f\"{backend:<20} ${cost:<9.4f} {time:<9.2f}s {status:<10}\")\n",
    "\n",
    "print(\"\\nüìä Task Type Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Task Type':<15} {'Cost':<10} {'Time':<10} {'Status':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for task_type, result in task_results.items():\n",
    "    if result:\n",
    "        cost = result['metrics']['cost']\n",
    "        time = result['metrics']['time_taken']\n",
    "        status = \"‚úÖ Success\"\n",
    "    else:\n",
    "        cost = time = 0\n",
    "        status = \"‚ùå Failed\"\n",
    "    \n",
    "    print(f\"{task_type:<15} ${cost:<9.4f} {time:<9.2f}s {status:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Tips\n",
    "\n",
    "Based on our experiments, here are some best practices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° Best Practices for Advanced Prompt Optimization:\")\n",
    "print(\"\\n1. Backend Selection:\")\n",
    "print(\"   - Use 'simple_meta_prompt' for most tasks (faster, cheaper)\")\n",
    "print(\"   - Use 'dspy' for complex reasoning tasks\")\n",
    "\n",
    "print(\"\\n2. Task Type Selection:\")\n",
    "print(\"   - Choose the most specific task type for better results\")\n",
    "print(\"   - Use 'qa' for question-answering\")\n",
    "print(\"   - Use 'classification' for categorization tasks\")\n",
    "print(\"   - Use 'summarization' for text compression\")\n",
    "\n",
    "print(\"\\n3. Synthetic Data Size:\")\n",
    "print(\"   - Start with 3-5 examples for testing\")\n",
    "print(\"   - Use 10+ examples for production\")\n",
    "print(\"   - Balance between cost and quality\")\n",
    "\n",
    "print(\"\\n4. Model Parameters:\")\n",
    "print(\"   - Lower temperature (0.1-0.3) for consistent results\")\n",
    "print(\"   - Higher temperature (0.7-0.9) for creative tasks\")\n",
    "print(\"   - Adjust max_tokens based on expected output length\")\n",
    "\n",
    "print(\"\\n5. Cost Optimization:\")\n",
    "print(\"   - Use smaller models for testing (gpt-3.5-turbo)\")\n",
    "print(\"   - Use larger models for final optimization (gpt-4)\")\n",
    "print(\"   - Monitor costs with the metrics provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "\n",
    "‚úÖ **Different Backends**: Compared simple_meta_prompt vs dspy\n",
    "‚úÖ **Task Types**: Tested classification, summarization, translation, QA, and generation\n",
    "‚úÖ **Synthetic Data**: Explored different data generation sizes\n",
    "‚úÖ **Advanced Config**: Custom model parameters and training ratios\n",
    "‚úÖ **Performance Analysis**: Cost and time comparisons\n",
    "‚úÖ **Best Practices**: Guidelines for optimal usage\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Backend Choice**: `simple_meta_prompt` is generally faster and cheaper\n",
    "- **Task Specificity**: More specific task types yield better results\n",
    "- **Data Size**: 3-5 examples are good for testing, 10+ for production\n",
    "- **Cost Management**: Monitor costs and adjust parameters accordingly\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore metrics and evaluation in `03_metrics_evaluation.ipynb`\n",
    "- Learn advanced features in `04_advanced_features.ipynb`\n",
    "- Try batch processing with the scripts\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for more?** Check out the metrics evaluation notebook to understand how to measure prompt quality!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
