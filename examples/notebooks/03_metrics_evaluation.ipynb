{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Evaluation\n",
    "\n",
    "This notebook focuses on evaluating prompt optimization performance through:\n",
    "- Performance metrics analysis\n",
    "- Cost tracking and optimization\n",
    "- Quality assessment\n",
    "- Comparative analysis\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed the basic usage notebook first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import Promptomatic functions\n",
    "from promptomatix.main import process_input, generate_feedback, optimize_with_feedback\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance Metrics Analysis\n",
    "\n",
    "Let's analyze the performance metrics from prompt optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple optimizations to collect metrics\n",
    "test_cases = [\n",
    "    {\"input\": \"Classify sentiment as positive or negative\", \"task\": \"classification\"},\n",
    "    {\"input\": \"Summarize text in 2 sentences\", \"task\": \"summarization\"},\n",
    "    {\"input\": \"Extract key information\", \"task\": \"extraction\"},\n",
    "    {\"input\": \"Generate creative content\", \"task\": \"generation\"},\n",
    "    {\"input\": \"Answer questions accurately\", \"task\": \"qa\"}\n",
    "]\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nüîÑ Running optimization {i}/{len(test_cases)}: {case['task']}\")\n",
    "    \n",
    "    config = {\n",
    "        \"raw_input\": case['input'],\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model_api_key\": api_key,\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"backend\": \"simple_meta_prompt\",\n",
    "        \"synthetic_data_size\": 3,\n",
    "        \"task_type\": case['task']\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = process_input(**config)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        metrics = {\n",
    "            'task_type': case['task'],\n",
    "            'input': case['input'],\n",
    "            'cost': result['metrics']['cost'],\n",
    "            'time_taken': result['metrics']['time_taken'],\n",
    "            'synthetic_data_count': len(result['synthetic_data']),\n",
    "            'prompt_length': len(result['result']),\n",
    "            'session_id': result['session_id']\n",
    "        }\n",
    "        \n",
    "        metrics_data.append(metrics)\n",
    "        \n",
    "        print(f\"‚úÖ Completed - Cost: ${metrics['cost']:.4f}, Time: {metrics['time_taken']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        metrics_data.append({\n",
    "            'task_type': case['task'],\n",
    "            'input': case['input'],\n",
    "            'cost': 0,\n",
    "            'time_taken': 0,\n",
    "            'synthetic_data_count': 0,\n",
    "            'prompt_length': 0,\n",
    "            'session_id': None,\n",
    "            'error': str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for analysis\n",
    "df = pd.DataFrame(metrics_data)\n",
    "print(\"üìä Metrics Summary:\")\n",
    "print(df)\n",
    "\n",
    "# Calculate summary statistics\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "print(f\"Total Cost: ${df['cost'].sum():.4f}\")\n",
    "print(f\"Total Time: {df['time_taken'].sum():.2f} seconds\")\n",
    "print(f\"Average Cost per Task: ${df['cost'].mean():.4f}\")\n",
    "print(f\"Average Time per Task: {df['time_taken'].mean():.2f} seconds\")\n",
    "print(f\"Average Prompt Length: {df['prompt_length'].mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cost Tracking and Optimization\n",
    "\n",
    "Let's analyze costs across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cost variations with different synthetic data sizes\n",
    "cost_test_cases = [\n",
    "    {\"size\": 1, \"description\": \"Minimal data\"},\n",
    "    {\"size\": 3, \"description\": \"Standard data\"},\n",
    "    {\"size\": 5, \"description\": \"Extended data\"},\n",
    "    {\"size\": 10, \"description\": \"Comprehensive data\"}\n",
    "]\n",
    "\n",
    "cost_data = []\n",
    "\n",
    "for case in cost_test_cases:\n",
    "    print(f\"\\nüîÑ Testing with {case['size']} synthetic examples...\")\n",
    "    \n",
    "    config = {\n",
    "        \"raw_input\": \"Classify text sentiment\",\n",
    "        \"model_name\": \"gpt-3.5-turbo\",\n",
    "        \"model_api_key\": api_key,\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"backend\": \"simple_meta_prompt\",\n",
    "        \"synthetic_data_size\": case['size'],\n",
    "        \"task_type\": \"classification\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = process_input(**config)\n",
    "        \n",
    "        cost_data.append({\n",
    "            'synthetic_data_size': case['size'],\n",
    "            'description': case['description'],\n",
    "            'cost': result['metrics']['cost'],\n",
    "            'time_taken': result['metrics']['time_taken'],\n",
    "            'cost_per_example': result['metrics']['cost'] / case['size']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Cost: ${result['metrics']['cost']:.4f}, Cost per example: ${result['metrics']['cost'] / case['size']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        cost_data.append({\n",
    "            'synthetic_data_size': case['size'],\n",
    "            'description': case['description'],\n",
    "            'cost': 0,\n",
    "            'time_taken': 0,\n",
    "            'cost_per_example': 0,\n",
    "            'error': str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost analysis\n",
    "cost_df = pd.DataFrame(cost_data)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Total cost vs synthetic data size\n",
    "ax1.plot(cost_df['synthetic_data_size'], cost_df['cost'], 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Synthetic Data Size')\n",
    "ax1.set_ylabel('Total Cost ($)')\n",
    "ax1.set_title('Total Cost vs Synthetic Data Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cost per example vs synthetic data size\n",
    "ax2.plot(cost_df['synthetic_data_size'], cost_df['cost_per_example'], 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Synthetic Data Size')\n",
    "ax2.set_ylabel('Cost per Example ($)')\n",
    "ax2.set_title('Cost per Example vs Synthetic Data Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Cost Analysis Summary:\")\n",
    "print(cost_df[['synthetic_data_size', 'cost', 'cost_per_example', 'time_taken']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quality Assessment\n",
    "\n",
    "Let's assess the quality of optimized prompts and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a comprehensive optimization with feedback\n",
    "print(\"üîÑ Running comprehensive quality assessment...\")\n",
    "\n",
    "quality_config = {\n",
    "    \"raw_input\": \"Analyze customer feedback and extract key insights\",\n",
    "    \"model_name\": \"gpt-3.5-turbo\",\n",
    "    \"model_api_key\": api_key,\n",
    "    \"model_provider\": \"openai\",\n",
    "    \"backend\": \"simple_meta_prompt\",\n",
    "    \"synthetic_data_size\": 5,\n",
    "    \"task_type\": \"extraction\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Initial optimization\n",
    "    initial_result = process_input(**quality_config)\n",
    "    \n",
    "    print(\"‚úÖ Initial optimization completed\")\n",
    "    print(f\"üìù Optimized prompt: {initial_result['result']}\")\n",
    "    print(f\"üìä Synthetic data count: {len(initial_result['synthetic_data'])}\")\n",
    "    \n",
    "    # Generate feedback\n",
    "    print(\"\\nüîÑ Generating feedback...\")\n",
    "    feedback_result = generate_feedback(\n",
    "        optimized_prompt=initial_result['result'],\n",
    "        input_fields=initial_result['input_fields'],\n",
    "        output_fields=initial_result['output_fields'],\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        model_api_key=api_key,\n",
    "        synthetic_data=initial_result['synthetic_data'],\n",
    "        session_id=initial_result['session_id']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Feedback generated\")\n",
    "    print(f\"üìã Comprehensive feedback: {feedback_result['comprehensive_feedback'][:200]}...\")\n",
    "    \n",
    "    # Optimize with feedback\n",
    "    print(\"\\nüîÑ Optimizing with feedback...\")\n",
    "    improved_result = optimize_with_feedback(initial_result['session_id'])\n",
    "    \n",
    "    print(\"‚úÖ Optimization with feedback completed\")\n",
    "    print(f\"üìù Improved prompt: {improved_result['result']}\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    quality_metrics = {\n",
    "        'initial_prompt_length': len(initial_result['result']),\n",
    "        'improved_prompt_length': len(improved_result['result']),\n",
    "        'initial_cost': initial_result['metrics']['cost'],\n",
    "        'feedback_cost': 0,  # Would need to track this separately\n",
    "        'improvement_cost': improved_result['metrics']['cost'],\n",
    "        'total_cost': initial_result['metrics']['cost'] + improved_result['metrics']['cost'],\n",
    "        'synthetic_data_quality': len(initial_result['synthetic_data']),\n",
    "        'feedback_count': len(feedback_result['individual_feedbacks'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä Quality Metrics:\")\n",
    "    for key, value in quality_metrics.items():\n",
    "        if 'cost' in key:\n",
    "            print(f\"  {key}: ${value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Quality assessment failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis\n",
    "\n",
    "Let's compare different approaches and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different task types performance\n",
    "print(\"üìä Task Type Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Task Type':<15} {'Cost ($)':<10} {'Time (s)':<10} {'Prompt Length':<15} {'Data Count':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"{row['task_type']:<15} {row['cost']:<10.4f} {row['time_taken']:<10.2f} {row['prompt_length']:<15} {row['synthetic_data_count']:<12}\")\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "df['cost_efficiency'] = df['cost'] / df['synthetic_data_count']\n",
    "df['time_efficiency'] = df['time_taken'] / df['synthetic_data_count']\n",
    "\n",
    "print(f\"\\nüìà Efficiency Analysis:\")\n",
    "print(f\"Most Cost Efficient: {df.loc[df['cost_efficiency'].idxmin(), 'task_type']}\")\n",
    "print(f\"Most Time Efficient: {df.loc[df['time_efficiency'].idxmin(), 'task_type']}\")\n",
    "print(f\"Longest Prompts: {df.loc[df['prompt_length'].idxmax(), 'task_type']}\")\n",
    "print(f\"Shortest Prompts: {df.loc[df['prompt_length'].idxmin(), 'task_type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparative analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cost by task type\n",
    "task_costs = df.groupby('task_type')['cost'].mean()\n",
    "ax1.bar(task_costs.index, task_costs.values, color='skyblue')\n",
    "ax1.set_title('Average Cost by Task Type')\n",
    "ax1.set_ylabel('Cost ($)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Time by task type\n",
    "task_times = df.groupby('task_type')['time_taken'].mean()\n",
    "ax2.bar(task_times.index, task_times.values, color='lightcoral')\n",
    "ax2.set_title('Average Time by Task Type')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prompt length by task type\n",
    "task_lengths = df.groupby('task_type')['prompt_length'].mean()\n",
    "ax3.bar(task_lengths.index, task_lengths.values, color='lightgreen')\n",
    "ax3.set_title('Average Prompt Length by Task Type')\n",
    "ax3.set_ylabel('Characters')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost vs Time scatter\n",
    "ax4.scatter(df['time_taken'], df['cost'], c=df['synthetic_data_count'], cmap='viridis', s=100)\n",
    "ax4.set_xlabel('Time (seconds)')\n",
    "ax4.set_ylabel('Cost ($)')\n",
    "ax4.set_title('Cost vs Time (colored by data count)')\n",
    "plt.colorbar(ax4.collections[0], ax=ax4, label='Synthetic Data Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendations and Best Practices\n",
    "\n",
    "Based on our analysis, here are recommendations for optimal usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° Recommendations Based on Metrics Analysis:\")\n",
    "print(\"\\n1. Cost Optimization:\")\n",
    "print(f\"   - Most cost-efficient task: {df.loc[df['cost_efficiency'].idxmin(), 'task_type']}\")\n",
    "print(f\"   - Average cost per task: ${df['cost'].mean():.4f}\")\n",
    "print(\"   - Use 3-5 synthetic examples for testing\")\n",
    "print(\"   - Scale up to 10+ examples for production\")\n",
    "\n",
    "print(\"\\n2. Time Optimization:\")\n",
    "print(f\"   - Most time-efficient task: {df.loc[df['time_efficiency'].idxmin(), 'task_type']}\")\n",
    "print(f\"   - Average time per task: {df['time_taken'].mean():.2f} seconds\")\n",
    "print(\"   - Use simple_meta_prompt backend for speed\")\n",
    "print(\"   - Consider parallel processing for batch tasks\")\n",
    "\n",
    "print(\"\\n3. Quality Optimization:\")\n",
    "print(f\"   - Average prompt length: {df['prompt_length'].mean():.0f} characters\")\n",
    "print(\"   - Longer prompts generally provide more detail\")\n",
    "print(\"   - Use feedback generation for quality improvement\")\n",
    "print(\"   - Balance prompt length with clarity\")\n",
    "\n",
    "print(\"\\n4. Task-Specific Recommendations:\")\n",
    "for task_type in df['task_type'].unique():\n",
    "    task_data = df[df['task_type'] == task_type].iloc[0]\n",
    "    print(f\"   - {task_type}: ${task_data['cost']:.4f} cost, {task_data['time_taken']:.2f}s time\")\n",
    "\n",
    "print(\"\\n5. Monitoring and Tracking:\")\n",
    "print(\"   - Track costs per task type\")\n",
    "print(\"   - Monitor time efficiency\")\n",
    "print(\"   - Evaluate prompt quality with feedback\")\n",
    "print(\"   - Use session IDs for result tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we conducted comprehensive metrics and evaluation analysis:\n",
    "\n",
    "‚úÖ **Performance Metrics**: Analyzed cost, time, and efficiency across task types\n",
    "‚úÖ **Cost Tracking**: Explored cost variations with different configurations\n",
    "‚úÖ **Quality Assessment**: Evaluated prompt quality and improvement processes\n",
    "‚úÖ **Comparative Analysis**: Compared different approaches and configurations\n",
    "‚úÖ **Recommendations**: Provided data-driven best practices\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- **Cost Efficiency**: Varies significantly by task type\n",
    "- **Time Efficiency**: Generally consistent across tasks\n",
    "- **Quality Improvement**: Feedback-based optimization adds value\n",
    "- **Scalability**: Synthetic data size affects cost linearly\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Explore advanced features in `04_advanced_features.ipynb`\n",
    "- Implement custom metrics tracking\n",
    "- Set up automated monitoring systems\n",
    "- Optimize for your specific use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for advanced features?** Check out the advanced features notebook to learn about custom configurations and integrations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
