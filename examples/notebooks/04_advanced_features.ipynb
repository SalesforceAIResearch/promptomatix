{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Features\n",
    "\n",
    "This notebook explores advanced features of Promptomatix including:\n",
    "- Custom configurations and parameters\n",
    "- Batch processing and automation\n",
    "- Integration examples\n",
    "- Advanced optimization strategies\n",
    "- Performance monitoring and debugging\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have completed the previous notebooks first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import Promptomatix functions\n",
    "from promptomatix.main import process_input, generate_feedback, optimize_with_feedback\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"✅ Advanced features setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Configurations and Parameters\n",
    "\n",
    "Let's explore advanced configuration options for fine-tuning optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced configuration with custom parameters\n",
    "advanced_configs = {\n",
    "    \"conservative\": {\n",
    "        \"description\": \"Conservative approach - low cost, basic optimization\",\n",
    "        \"config\": {\n",
    "            \"raw_input\": \"Classify text sentiment\",\n",
    "            \"model_name\": \"gpt-3.5-turbo\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"simple_meta_prompt\",\n",
    "            \"synthetic_data_size\": 2,\n",
    "            \"task_type\": \"classification\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 1000,\n",
    "            \"train_ratio\": 0.7\n",
    "        }\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"description\": \"Balanced approach - moderate cost and quality\",\n",
    "        \"config\": {\n",
    "            \"raw_input\": \"Classify text sentiment\",\n",
    "            \"model_name\": \"gpt-3.5-turbo\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"simple_meta_prompt\",\n",
    "            \"synthetic_data_size\": 5,\n",
    "            \"task_type\": \"classification\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 1500,\n",
    "            \"train_ratio\": 0.8\n",
    "        }\n",
    "    },\n",
    "    \"aggressive\": {\n",
    "        \"description\": \"Aggressive approach - high quality, higher cost\",\n",
    "        \"config\": {\n",
    "            \"raw_input\": \"Classify text sentiment\",\n",
    "            \"model_name\": \"gpt-4\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"dspy\",\n",
    "            \"synthetic_data_size\": 10,\n",
    "            \"task_type\": \"classification\",\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"train_ratio\": 0.9\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "config_results = {}\n",
    "\n",
    "for approach, details in advanced_configs.items():\n",
    "    print(f\"\\n🔄 Testing {approach} approach...\")\n",
    "    print(f\"Description: {details['description']}\")\n",
    "    \n",
    "    try:\n",
    "        result = process_input(**details['config'])\n",
    "        config_results[approach] = result\n",
    "        \n",
    "        print(f\"✅ {approach} completed\")\n",
    "        print(f\"📝 Prompt length: {len(result['result'])} characters\")\n",
    "        print(f\"💰 Cost: ${result['metrics']['cost']:.4f}\")\n",
    "        print(f\"⏱️  Time: {result['metrics']['time_taken']:.2f}s\")\n",
    "        print(f\"📊 Synthetic data: {len(result['synthetic_data'])} examples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {approach} failed: {str(e)}\")\n",
    "        config_results[approach] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Processing and Automation\n",
    "\n",
    "Let's implement batch processing for multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample prompts from the data file\n",
    "def load_sample_prompts():\n",
    "    \"\"\"Load sample prompts for batch processing.\"\"\"\n",
    "    prompts = []\n",
    "    try:\n",
    "        with open('../data/sample_prompts.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith('#'):\n",
    "                    prompts.append(line)\n",
    "    except FileNotFoundError:\n",
    "        # Fallback prompts if file not found\n",
    "        prompts = [\n",
    "            \"Classify customer feedback as positive, negative, or neutral\",\n",
    "            \"Summarize a long article in 3 sentences\",\n",
    "            \"Extract key information from customer reviews\",\n",
    "            \"Generate creative marketing copy for a product\",\n",
    "            \"Answer questions about a given text accurately\"\n",
    "        ]\n",
    "    return prompts[:5]  # Limit to 5 for demonstration\n",
    "\n",
    "sample_prompts = load_sample_prompts()\n",
    "print(f\"📝 Loaded {len(sample_prompts)} sample prompts for batch processing\")\n",
    "\n",
    "# Batch processing function\n",
    "def process_batch(prompts: List[str], max_workers: int = 2) -> List[Dict]:\n",
    "    \"\"\"Process multiple prompts in parallel.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    def process_single(prompt: str) -> Dict:\n",
    "        config = {\n",
    "            \"raw_input\": prompt,\n",
    "            \"model_name\": \"gpt-3.5-turbo\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"simple_meta_prompt\",\n",
    "            \"synthetic_data_size\": 3,\n",
    "            \"task_type\": \"classification\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            result = process_input(**config)\n",
    "            return {\"success\": True, \"result\": result, \"prompt\": prompt}\n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"error\": str(e), \"prompt\": prompt}\n",
    "    \n",
    "    print(f\"🔄 Processing {len(prompts)} prompts with {max_workers} workers...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single, prompt) for prompt in prompts]\n",
    "        \n",
    "        for i, future in enumerate(futures, 1):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"✅ {i}/{len(prompts)}: {result['prompt'][:50]}...\")\n",
    "            else:\n",
    "                print(f\"❌ {i}/{len(prompts)}: {result['error']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch processing\n",
    "batch_results = process_batch(sample_prompts, max_workers=2)\n",
    "\n",
    "# Analyze batch results\n",
    "successful_results = [r for r in batch_results if r['success']]\n",
    "failed_results = [r for r in batch_results if not r['success']]\n",
    "\n",
    "print(f\"\\n📊 Batch Processing Summary:\")\n",
    "print(f\"Total prompts: {len(batch_results)}\")\n",
    "print(f\"Successful: {len(successful_results)}\")\n",
    "print(f\"Failed: {len(failed_results)}\")\n",
    "print(f\"Success rate: {len(successful_results)/len(batch_results)*100:.1f}%\")\n",
    "\n",
    "if successful_results:\n",
    "    total_cost = sum(r['result']['metrics']['cost'] for r in successful_results)\n",
    "    total_time = sum(r['result']['metrics']['time_taken'] for r in successful_results)\n",
    "    print(f\"Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration Examples\n",
    "\n",
    "Let's explore how to integrate Promptomatix with other systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Integration with a simple API wrapper\n",
    "class PromptomatixAPI:\n",
    "    \"\"\"Simple API wrapper for Promptomatix integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.session_history = {}\n",
    "    \n",
    "    def optimize_prompt(self, raw_input: str, task_type: str = \"classification\", **kwargs) -> Dict:\n",
    "        \"\"\"Optimize a prompt with error handling and logging.\"\"\"\n",
    "        try:\n",
    "            config = {\n",
    "                \"raw_input\": raw_input,\n",
    "                \"model_name\": kwargs.get(\"model_name\", \"gpt-3.5-turbo\"),\n",
    "                \"model_api_key\": self.api_key,\n",
    "                \"model_provider\": \"openai\",\n",
    "                \"backend\": kwargs.get(\"backend\", \"simple_meta_prompt\"),\n",
    "                \"synthetic_data_size\": kwargs.get(\"synthetic_data_size\", 3),\n",
    "                \"task_type\": task_type\n",
    "            }\n",
    "            \n",
    "            result = process_input(**config)\n",
    "            \n",
    "            # Store in session history\n",
    "            self.session_history[result['session_id']] = {\n",
    "                'raw_input': raw_input,\n",
    "                'optimized_prompt': result['result'],\n",
    "                'task_type': task_type,\n",
    "                'timestamp': time.time(),\n",
    "                'metrics': result['metrics']\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'session_id': result['session_id'],\n",
    "                'optimized_prompt': result['result'],\n",
    "                'metrics': result['metrics']\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'raw_input': raw_input\n",
    "            }\n",
    "    \n",
    "    def get_session_history(self) -> Dict:\n",
    "        \"\"\"Get session history.\"\"\"\n",
    "        return self.session_history\n",
    "    \n",
    "    def get_session_stats(self) -> Dict:\n",
    "        \"\"\"Get session statistics.\"\"\"\n",
    "        if not self.session_history:\n",
    "            return {'total_sessions': 0, 'total_cost': 0, 'total_time': 0}\n",
    "        \n",
    "        total_cost = sum(session['metrics']['cost'] for session in self.session_history.values())\n",
    "        total_time = sum(session['metrics']['time_taken'] for session in self.session_history.values())\n",
    "        \n",
    "        return {\n",
    "            'total_sessions': len(self.session_history),\n",
    "            'total_cost': total_cost,\n",
    "            'total_time': total_time,\n",
    "            'average_cost_per_session': total_cost / len(self.session_history)\n",
    "        }\n",
    "\n",
    "# Test the API wrapper\n",
    "print(\"🔄 Testing API wrapper integration...\")\n",
    "\n",
    "api_wrapper = PromptomatixAPI(api_key)\n",
    "\n",
    "# Test multiple optimizations\n",
    "test_inputs = [\n",
    "    \"Analyze customer sentiment\",\n",
    "    \"Summarize technical documentation\",\n",
    "    \"Extract key metrics from reports\"\n",
    "]\n",
    "\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n📝 Test {i}: {test_input}\")\n",
    "    result = api_wrapper.optimize_prompt(test_input, task_type=\"classification\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"✅ Success - Session: {result['session_id']}\")\n",
    "        print(f\"💰 Cost: ${result['metrics']['cost']:.4f}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed: {result['error']}\")\n",
    "\n",
    "# Get session statistics\n",
    "stats = api_wrapper.get_session_stats()\n",
    "print(f\"\\n📊 API Wrapper Statistics:\")\n",
    "print(f\"Total sessions: {stats['total_sessions']}\")\n",
    "print(f\"Total cost: ${stats['total_cost']:.4f}\")\n",
    "print(f\"Total time: {stats['total_time']:.2f} seconds\")\n",
    "print(f\"Average cost per session: ${stats['average_cost_per_session']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Optimization Strategies\n",
    "\n",
    "Let's explore advanced strategies for prompt optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-stage optimization strategy\n",
    "def multi_stage_optimization(raw_input: str, stages: int = 3) -> List[Dict]:\n",
    "    \"\"\"Perform multi-stage optimization with progressive refinement.\"\"\"\n",
    "    results = []\n",
    "    current_input = raw_input\n",
    "    \n",
    "    for stage in range(stages):\n",
    "        print(f\"\\n🔄 Stage {stage + 1}/{stages}: Optimizing...\")\n",
    "        \n",
    "        # Adjust parameters based on stage\n",
    "        synthetic_data_size = 2 + stage * 2  # Increase data size progressively\n",
    "        temperature = 0.1 + stage * 0.2  # Increase creativity progressively\n",
    "        \n",
    "        config = {\n",
    "            \"raw_input\": current_input,\n",
    "            \"model_name\": \"gpt-3.5-turbo\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"simple_meta_prompt\",\n",
    "            \"synthetic_data_size\": synthetic_data_size,\n",
    "            \"task_type\": \"classification\",\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            result = process_input(**config)\n",
    "            results.append({\n",
    "                'stage': stage + 1,\n",
    "                'input': current_input,\n",
    "                'output': result['result'],\n",
    "                'metrics': result['metrics'],\n",
    "                'synthetic_data_count': len(result['synthetic_data'])\n",
    "            })\n",
    "            \n",
    "            # Use output as input for next stage\n",
    "            current_input = result['result']\n",
    "            \n",
    "            print(f\"✅ Stage {stage + 1} completed\")\n",
    "            print(f\"📝 Output length: {len(result['result'])} characters\")\n",
    "            print(f\"💰 Cost: ${result['metrics']['cost']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Stage {stage + 1} failed: {str(e)}\")\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test multi-stage optimization\n",
    "print(\"🚀 Testing Multi-Stage Optimization Strategy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "multi_stage_results = multi_stage_optimization(\n",
    "    \"Classify customer feedback with detailed reasoning\", \n",
    "    stages=3\n",
    ")\n",
    "\n",
    "# Analyze multi-stage results\n",
    "if multi_stage_results:\n",
    "    print(f\"\\n📊 Multi-Stage Optimization Summary:\")\n",
    "    print(f\"Total stages completed: {len(multi_stage_results)}\")\n",
    "    \n",
    "    total_cost = sum(r['metrics']['cost'] for r in multi_stage_results)\n",
    "    total_time = sum(r['metrics']['time_taken'] for r in multi_stage_results)\n",
    "    \n",
    "    print(f\"Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    print(f\"\\n📈 Progressive Improvement:\")\n",
    "    for i, result in enumerate(multi_stage_results):\n",
    "        print(f\"Stage {result['stage']}: {len(result['output'])} chars, ${result['metrics']['cost']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring and Debugging\n",
    "\n",
    "Let's implement performance monitoring and debugging tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring class\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor and track performance metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.operations = []\n",
    "        self.errors = []\n",
    "    \n",
    "    def track_operation(self, operation_type: str, start_time: float, end_time: float, \n",
    "                       cost: float, success: bool, metadata: Dict = None):\n",
    "        \"\"\"Track an operation.\"\"\"\n",
    "        operation = {\n",
    "            'type': operation_type,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'duration': end_time - start_time,\n",
    "            'cost': cost,\n",
    "            'success': success,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.operations.append(operation)\n",
    "        \n",
    "        if not success:\n",
    "            self.errors.append(operation)\n",
    "    \n",
    "    def get_performance_summary(self) -> Dict:\n",
    "        \"\"\"Get performance summary.\"\"\"\n",
    "        if not self.operations:\n",
    "            return {'total_operations': 0, 'success_rate': 0, 'total_cost': 0}\n",
    "        \n",
    "        successful_ops = [op for op in self.operations if op['success']]\n",
    "        total_cost = sum(op['cost'] for op in self.operations)\n",
    "        total_time = sum(op['duration'] for op in self.operations)\n",
    "        \n",
    "        return {\n",
    "            'total_operations': len(self.operations),\n",
    "            'successful_operations': len(successful_ops),\n",
    "            'failed_operations': len(self.errors),\n",
    "            'success_rate': len(successful_ops) / len(self.operations),\n",
    "            'total_cost': total_cost,\n",
    "            'total_time': total_time,\n",
    "            'average_cost_per_operation': total_cost / len(self.operations),\n",
    "            'average_time_per_operation': total_time / len(self.operations)\n",
    "        }\n",
    "    \n",
    "    def get_error_analysis(self) -> Dict:\n",
    "        \"\"\"Analyze errors.\"\"\"\n",
    "        if not self.errors:\n",
    "            return {'total_errors': 0, 'error_types': {}}\n",
    "        \n",
    "        error_types = {}\n",
    "        for error in self.errors:\n",
    "            error_type = error['metadata'].get('error_type', 'unknown')\n",
    "            error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_errors': len(self.errors),\n",
    "            'error_types': error_types\n",
    "        }\n",
    "\n",
    "# Test performance monitoring\n",
    "print(\"🔍 Testing Performance Monitoring...\")\n",
    "\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "# Simulate some operations\n",
    "test_operations = [\n",
    "    {\"input\": \"Classify sentiment\", \"type\": \"optimization\"},\n",
    "    {\"input\": \"Summarize text\", \"type\": \"optimization\"},\n",
    "    {\"input\": \"Invalid input\", \"type\": \"optimization\"}  # This will fail\n",
    "]\n",
    "\n",
    "for i, op in enumerate(test_operations):\n",
    "    print(f\"\\n🔄 Operation {i + 1}: {op['input']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if \"Invalid\" in op['input']:\n",
    "            raise ValueError(\"Invalid input detected\")\n",
    "        \n",
    "        config = {\n",
    "            \"raw_input\": op['input'],\n",
    "            \"model_name\": \"gpt-3.5-turbo\",\n",
    "            \"model_api_key\": api_key,\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"backend\": \"simple_meta_prompt\",\n",
    "            \"synthetic_data_size\": 2,\n",
    "            \"task_type\": \"classification\"\n",
    "        }\n",
    "        \n",
    "        result = process_input(**config)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        monitor.track_operation(\n",
    "            operation_type=op['type'],\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            cost=result['metrics']['cost'],\n",
    "            success=True,\n",
    "            metadata={'input': op['input']}\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Success - Cost: ${result['metrics']['cost']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        \n",
    "        monitor.track_operation(\n",
    "            operation_type=op['type'],\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            cost=0,\n",
    "            success=False,\n",
    "            metadata={'input': op['input'], 'error_type': type(e).__name__, 'error': str(e)}\n",
    "        )\n",
    "        \n",
    "        print(f\"❌ Failed: {str(e)}\")\n",
    "\n",
    "# Display performance summary\n",
    "summary = monitor.get_performance_summary()\n",
    "error_analysis = monitor.get_error_analysis()\n",
    "\n",
    "print(f\"\\n📊 Performance Summary:\")\n",
    "print(f\"Total operations: {summary['total_operations']}\")\n",
    "print(f\"Success rate: {summary['success_rate']:.1%}\")\n",
    "print(f\"Total cost: ${summary['total_cost']:.4f}\")\n",
    "print(f\"Total time: {summary['total_time']:.2f} seconds\")\n",
    "print(f\"Average cost per operation: ${summary['average_cost_per_operation']:.4f}\")\n",
    "\n",
    "print(f\"\\n🔍 Error Analysis:\")\n",
    "print(f\"Total errors: {error_analysis['total_errors']}\")\n",
    "for error_type, count in error_analysis['error_types'].items():\n",
    "    print(f\"  {error_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Recommendations\n",
    "\n",
    "Based on our advanced feature exploration, here are best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💡 Advanced Features Best Practices:\")\n",
    "print(\"\\n1. Configuration Strategy:\")\n",
    "print(\"   - Start with conservative settings for testing\")\n",
    "print(\"   - Use balanced settings for production\")\n",
    "print(\"   - Apply aggressive settings only for critical tasks\")\n",
    "print(\"   - Monitor costs and adjust parameters accordingly\")\n",
    "\n",
    "print(\"\\n2. Batch Processing:\")\n",
    "print(\"   - Use ThreadPoolExecutor for parallel processing\")\n",
    "print(\"   - Limit max_workers to avoid API rate limits\")\n",
    "print(\"   - Implement proper error handling and retry logic\")\n",
    "print(\"   - Track progress and provide user feedback\")\n",
    "\n",
    "print(\"\\n3. Integration Patterns:\")\n",
    "print(\"   - Create wrapper classes for consistent API\")\n",
    "print(\"   - Implement session management and history tracking\")\n",
    "print(\"   - Add comprehensive error handling\")\n",
    "print(\"   - Provide statistics and monitoring capabilities\")\n",
    "\n",
    "print(\"\\n4. Advanced Optimization:\")\n",
    "print(\"   - Use multi-stage optimization for complex tasks\")\n",
    "print(\"   - Progressively increase synthetic data size\")\n",
    "print(\"   - Adjust temperature for creativity vs consistency\")\n",
    "print(\"   - Monitor quality improvements vs cost increases\")\n",
    "\n",
    "print(\"\\n5. Performance Monitoring:\")\n",
    "print(\"   - Track all operations with timing and cost\")\n",
    "print(\"   - Monitor success rates and error patterns\")\n",
    "print(\"   - Set up alerts for cost thresholds\")\n",
    "print(\"   - Analyze performance trends over time\")\n",
    "\n",
    "print(\"\\n6. Production Considerations:\")\n",
    "print(\"   - Implement rate limiting and backoff strategies\")\n",
    "print(\"   - Add comprehensive logging and debugging\")\n",
    "print(\"   - Set up monitoring and alerting systems\")\n",
    "print(\"   - Plan for scalability and cost management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored advanced features of Promptomatix:\n",
    "\n",
    "✅ **Custom Configurations**: Explored different optimization approaches\n",
    "✅ **Batch Processing**: Implemented parallel processing for multiple prompts\n",
    "✅ **Integration Examples**: Created API wrappers and session management\n",
    "✅ **Advanced Strategies**: Multi-stage optimization and progressive refinement\n",
    "✅ **Performance Monitoring**: Comprehensive tracking and debugging tools\n",
    "✅ **Best Practices**: Production-ready recommendations\n",
    "\n",
    "### Key Advanced Features:\n",
    "\n",
    "- **Multi-Stage Optimization**: Progressive refinement for complex tasks\n",
    "- **Batch Processing**: Efficient handling of multiple prompts\n",
    "- **API Integration**: Wrapper classes for consistent interfaces\n",
    "- **Performance Monitoring**: Comprehensive tracking and analysis\n",
    "- **Error Handling**: Robust error management and debugging\n",
    "\n",
    "### Production Readiness:\n",
    "\n",
    "The advanced features demonstrated here provide the foundation for:\n",
    "- Scalable prompt optimization systems\n",
    "- Production-ready integrations\n",
    "- Comprehensive monitoring and debugging\n",
    "- Cost-effective batch processing\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Implement these patterns in your own applications\n",
    "- Set up monitoring and alerting systems\n",
    "- Optimize for your specific use cases\n",
    "- Consider custom metrics and evaluation frameworks\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've now explored all the advanced features of Promptomatix. You're ready to build production-ready prompt optimization systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
